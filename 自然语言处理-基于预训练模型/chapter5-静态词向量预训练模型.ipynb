{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词表\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None):\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens = tokens + [\"<unk>\"]\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx['<unk>']\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, text, min_freq=1, reserved_tokens=None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() \\\n",
    "                        if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def save_vocab(vocab, path):\n",
    "    with open(path, 'w') as writer:\n",
    "        writer.write(\"\\n\".join(vocab.idx_to_token))\n",
    "\n",
    "\n",
    "def read_vocab(path):\n",
    "    with open(path, 'r') as f:\n",
    "        tokens = f.read().split('\\n')\n",
    "    return Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Constants\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOW_TOKEN = \"<bow>\"\n",
    "EOW_TOKEN = \"<eow>\"\n",
    "\n",
    "WEIGHT_INIT_RANGE = 0.1\n",
    "\n",
    "def load_reuters():\n",
    "    from nltk.corpus import reuters\n",
    "    text = reuters.sents()\n",
    "    # lowercase (optional)\n",
    "    text = [[word.lower() for word in sentence] for sentence in text]\n",
    "    vocab = Vocab.build(text, reserved_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN])\n",
    "    corpus = [vocab.convert_tokens_to_ids(sentence) for sentence in text]\n",
    "\n",
    "    return corpus, vocab\n",
    "\n",
    "def save_pretrained(vocab, embeds, save_path):\n",
    "    \"\"\"\n",
    "    Save pretrained token vectors in a unified format, where the first line\n",
    "    specifies the `number_of_tokens` and `embedding_dim` followed with all\n",
    "    token vectors, one token per line.\n",
    "    \"\"\"\n",
    "    with open(save_path, \"w\") as writer:\n",
    "        writer.write(f\"{embeds.shape[0]} {embeds.shape[1]}\\n\")\n",
    "        for idx, token in enumerate(vocab.idx_to_token):\n",
    "            vec = \" \".join([\"{:.4f}\".format(x) for x in embeds[idx]])\n",
    "            writer.write(f\"{token} {vec}\\n\")\n",
    "    print(f\"Pretrained embeddings saved to: {save_path}\")\n",
    "\n",
    "def load_pretrained(load_path):\n",
    "    with open(load_path, \"r\") as fin:\n",
    "        # Optional: depending on the specific format of pretrained vector file\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        tokens = []\n",
    "        embeds = []\n",
    "        for line in fin:\n",
    "            line = line.rstrip().split(' ')\n",
    "            token, embed = line[0], list(map(float, line[1:]))\n",
    "            tokens.append(token)\n",
    "            embeds.append(embed)\n",
    "        vocab = Vocab(tokens)\n",
    "        embeds = torch.tensor(embeds, dtype=torch.float)\n",
    "    return vocab, embeds\n",
    "\n",
    "def get_loader(dataset, batch_size, shuffle=True):\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"embedding\" not in name:\n",
    "            torch.nn.init.uniform_(\n",
    "                param, a=-WEIGHT_INIT_RANGE, b=WEIGHT_INIT_RANGE\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据\n",
    "class CbowDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, context_size=2):\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "\n",
    "            # 如果句子长度不足以构建（上下文+目标词）训练样本，则跳过\n",
    "            if len(sentence) < context_size * 2 + 1:\n",
    "                continue\n",
    "            \n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                # 模型输入：左右各分别取context_size长度的上下文\n",
    "                context = sentence[i-context_size:i] + sentence[i+1:i+context_size+1]\n",
    "                # 模型输出：当前词\n",
    "                target = sentence[i]\n",
    "                self.data.append((context, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        inputs = torch.tensor([ex[0] for ex in examples])\n",
    "        targets = torch.tensor([ex[1] for ex in examples])\n",
    "        return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型\n",
    "class CbowModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CbowModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) # 词向量层\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size, bias=False) # 输出层\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs)\n",
    "        hidden = embeds.mean(dim=1) # 计算隐含层：对上下文词向量求平均\n",
    "        output = self.output(hidden)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd3b3a899a45ec9b2bda008a64da0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3012eb8cadf94976af16528d5d14d28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10278.96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0aa949435544a6920ce854d6365a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8293.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08291301c167486faf59e175b7dcc76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7738.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348a7b291129402e8709d793c5c5f2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7404.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4b345147db4293b97b561bbdd5cd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7165.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5497452ce7a84eaaa433d41cc1406f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6978.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a59754bed4a1f81d05d823ce07de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6825.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a66349dda54641810e6e5ec266fa11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6696.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22cd7a408544ada52e9eb7f26fae92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6584.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76511d2a3cc14f2b98de45f2782cfa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/1574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6485.60\n",
      "Pretrained embeddings saved to: cbow.vec\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "# 读取文本数据，构建CBOW模型训练数据集\n",
    "corpus, vocab = load_reuters()\n",
    "dataset = CbowDataset(corpus, vocab, context_size=context_size)\n",
    "data_loader = get_loader(dataset, batch_size)\n",
    "\n",
    "nll_loss = nn.NLLLoss() # torch.nn.CrossEntropyLoss相当于softmax + log + nllloss\n",
    "# 构建CBOW模型，并加载至device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CbowModel(len(vocab), embedding_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 保存词向量（model.embeddings）\n",
    "save_pretrained(vocab, model.embeddings.weight.data, \"cbow.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, context_size=2):\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            for i in range(1, len(sentence)-1):\n",
    "                # 模型输入：当前词\n",
    "                w = sentence[i]\n",
    "                # 模型输出：一定窗口大小内的上下文\n",
    "                left_context_index = max(0, i - context_size)\n",
    "                right_context_index = min(len(sentence), i + context_size)\n",
    "                context = sentence[left_context_index:i] + sentence[i+1:right_context_index+1]\n",
    "                self.data.extend([(w, c) for c in context])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        inputs = torch.tensor([ex[0] for ex in examples])\n",
    "        targets = torch.tensor([ex[1] for ex in examples])\n",
    "        return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output = self.output(embeds)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c73e576fe6a403294213583c68e1375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e844684f4544649d937ddb5bde85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 43176.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b64fb92bae4401daaaf06862586b2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 40015.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c445b4adab7e4ab2a8799bb4152615c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 39360.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8497c4395d6247078d85f0cd753ababd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38967.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafdbef94bf94e96aa068513451de5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38692.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e340f8728241048f49d637ac398dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38484.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79665d47c8ea4e06b2abd1f1951a2888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38317.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf405c18462e4417a9d6d18c41e0522b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38181.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2801820db64455ab6eae690482cf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 38065.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226a596de0904e7492d92646687d35cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/6616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 37965.89\n",
      "Pretrained embeddings saved to: skipgram.vec\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "# 读取文本数据，构建Skip-gram模型训练数据集\n",
    "corpus, vocab = load_reuters()\n",
    "dataset = SkipGramDataset(corpus, vocab, context_size=context_size)\n",
    "data_loader = get_loader(dataset, batch_size)\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "# 构建Skip-gram模型，并加载至device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SkipGramModel(len(vocab), embedding_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 保存词向量（model.embeddings）\n",
    "save_pretrained(vocab, model.embeddings.weight.data, \"skipgram.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于负采样的Skip-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNSDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, context_size=2, n_negatives=5, ns_dist=None):\n",
    "\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        self.pad = vocab[PAD_TOKEN]\n",
    "\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            for i in range(1, len(sentence) - 1):\n",
    "                # 模型输入：(w, context) ；输出为0/1，表示context是否为负样本\n",
    "                w = sentence[i]\n",
    "                left_context_index = max(0, i - context_size)\n",
    "                right_context_index = min(len(sentence), i + context_size)\n",
    "                context = sentence[left_context_index:i] + sentence[i+1:right_context_index+1]\n",
    "                context += [self.pad] * (2 * context_size - len(context))\n",
    "                self.data.append((w, context))\n",
    "\n",
    "        # 负样本数量\n",
    "        self.n_negatives = n_negatives\n",
    "        # 负采样分布：若参数ns_dist为None，则使用uniform分布\n",
    "        self.ns_dist = ns_dist if ns_dist is not None else torch.ones(len(vocab))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        words = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "        contexts = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "        batch_size, context_size = contexts.shape\n",
    "        neg_contexts = []\n",
    "\n",
    "        # 对batch内的样本分别进行负采样\n",
    "        for i in range(batch_size):\n",
    "            # 保证负样本不包含当前样本中的context\n",
    "            ns_dist = self.ns_dist.index_fill(0, contexts[i], .0) # 填0防止抽到自己\n",
    "            neg_contexts.append(torch.multinomial(ns_dist, self.n_negatives * context_size, replacement=True)) # 有放回的采样，input中值为0的元素永远不会被抽到\n",
    "        neg_contexts = torch.stack(neg_contexts, dim=0) # list包tensor，变为tensor包list\n",
    "        return words, contexts, neg_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNSModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SGNSModel, self).__init__()\n",
    "        self.w_embeddings = nn.Embedding(vocab_size, embedding_dim) # 词嵌入\n",
    "        self.c_embeddings = nn.Embedding(vocab_size, embedding_dim) # 上下文嵌入\n",
    "\n",
    "    def forward_w(self, words):\n",
    "        w_embeds = self.w_embeddings(words)\n",
    "        return w_embeds\n",
    "\n",
    "    def forward_c(self, contexts):\n",
    "        c_embeds = self.c_embeddings(contexts)\n",
    "        return c_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_distribution(corpus, vocab_size):\n",
    "    # 从给定语料中统计unigram概率分布\n",
    "    token_counts = torch.tensor([0] * vocab_size)\n",
    "    total_count = 0\n",
    "    for sentence in corpus:\n",
    "        total_count += len(sentence)\n",
    "        for token in sentence:\n",
    "            token_counts[token] += 1\n",
    "    unigram_dist = torch.div(token_counts.float(), total_count) # torch.div 张量和标量做逐元素除法\n",
    "    return unigram_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "n_negatives = 10\n",
    "\n",
    "# 读取文本数据\n",
    "corpus, vocab = load_reuters()\n",
    "# 计算unigram概率分布\n",
    "unigram_dist = get_unigram_distribution(corpus, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0c3c095bd7465eb44bf34bc65667aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据unigram分布计算负采样分布: p(w)**0.75\n",
    "negative_sampling_dist = unigram_dist ** 0.75\n",
    "negative_sampling_dist /= negative_sampling_dist.sum()\n",
    "# 构建SGNS训练数据集\n",
    "dataset = SGNSDataset(\n",
    "    corpus,\n",
    "    vocab,\n",
    "    context_size=context_size,\n",
    "    n_negatives=n_negatives,\n",
    "    ns_dist=negative_sampling_dist\n",
    ")\n",
    "data_loader = get_loader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c0e36d14f8442d8c956e9bfe87a20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 32961.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfda386af494986a5ddc2456d8533dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11422.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5e68728fd44d23b157103ca9394bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7378.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0ace8be645425d9df7d1e1ef5a378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6034.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe250936924190a50ded46a151e370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5376.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319a15dc608a4181beadc366e2ffeed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4983.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e2e19228b84859b7096ec55a767687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4725.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5d52afee244dea9f758bac8f239387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4541.41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559b3ea42b64cd0b14459ba20595f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4403.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052ed1fd0a7a44d7b4595e19d236b559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/1681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4297.14\n",
      "Pretrained embeddings saved to: sgns.vec\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SGNSModel(len(vocab), embedding_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        words, contexts, neg_contexts = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = words.shape[0]\n",
    "        # 提取batch内词、上下文以及负样本的向量表示\n",
    "        word_embeds = model.forward_w(words).unsqueeze(dim=2)\n",
    "        context_embeds = model.forward_c(contexts)\n",
    "        neg_context_embeds = model.forward_c(neg_contexts)\n",
    "        # 正样本的分类（对数）似然\n",
    "        context_loss = F.logsigmoid(torch.bmm(context_embeds, word_embeds).squeeze(dim=2))\n",
    "        context_loss = context_loss.mean(dim=1)\n",
    "        # 负样本的分类（对数）似然\n",
    "        neg_context_loss = F.logsigmoid(torch.bmm(neg_context_embeds, word_embeds).squeeze(dim=2).neg())\n",
    "        neg_context_loss = neg_context_loss.view(batch_size, -1, n_negatives).sum(dim=2)\n",
    "        neg_context_loss = neg_context_loss.mean(dim=1)\n",
    "        # 损失：负对数似然\n",
    "        loss = -(context_loss + neg_context_loss).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 合并词嵌入矩阵与上下文嵌入矩阵，作为最终的预训练词向量\n",
    "combined_embeds = model.w_embeddings.weight + model.c_embeddings.weight\n",
    "save_pretrained(vocab, combined_embeds.data, \"sgns.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据\n",
    "class GloveDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, context_size=2):\n",
    "        # 记录词与上下文在给定语料中的共现次数\n",
    "        self.cooccur_counts = defaultdict(float)\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "\n",
    "            for i in range(1, len(sentence)-1):\n",
    "                w = sentence[i]\n",
    "                left_contexts = sentence[max(0, i - context_size):i]\n",
    "                right_contexts = sentence[i+1:min(len(sentence), i + context_size)+1]\n",
    "                # 共现次数随距离衰减: 1/d(w, c)\n",
    "                for k, c in enumerate(left_contexts[::-1]):\n",
    "                    self.cooccur_counts[(w, c)] += 1 / (k + 1) # k + 1距中心词的距离\n",
    "                for k, c in enumerate(right_contexts):\n",
    "                    self.cooccur_counts[(w, c)] += 1 / (k + 1)\n",
    "        self.data = [(w, c, count) for (w, c), count in self.cooccur_counts.items()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        words = torch.tensor([ex[0] for ex in examples])\n",
    "        contexts = torch.tensor([ex[1] for ex in examples])\n",
    "        counts = torch.tensor([ex[2] for ex in examples])\n",
    "        return (words, contexts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(GloveModel, self).__init__()\n",
    "        # 词嵌入及偏置向量\n",
    "        self.w_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.w_biases = nn.Embedding(vocab_size, 1)\n",
    "        # 上下文嵌入及偏置向量\n",
    "        self.c_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.c_biases = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward_w(self, words):\n",
    "        w_embeds = self.w_embeddings(words)\n",
    "        w_biases = self.w_biases(words)\n",
    "        return w_embeds, w_biases\n",
    "\n",
    "    def forward_c(self, contexts):\n",
    "        c_embeds = self.c_embeddings(contexts)\n",
    "        c_biases = self.c_biases(contexts)\n",
    "        return c_embeds, c_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abcff9470c7492aadc36d768e41048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "# 用以控制样本权重的超参数\n",
    "m_max = 100\n",
    "alpha = 0.75\n",
    "# 从文本数据中构建GloVe训练数据集\n",
    "corpus, vocab = load_reuters()\n",
    "dataset = GloveDataset(\n",
    "    corpus,\n",
    "    vocab,\n",
    "    context_size=context_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096a6431f7b6459f92a6aea6990a9b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20163172dd1d4a06a79a6288b40c73ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4331.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca073944b0a24f49b8d6601e63d5937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2996.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a556778aadb4ebb933f40ddf879510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2130.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fb6583286a4487817a0da99d6a21a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1539.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b498e4e83f35482db42d26a64d07382a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1128.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddbd5d2eb944a03b4286b23ef86038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 841.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602fae353c4b46ecb54e461348ea57e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 638.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1606d94f9bda481da71c20632be5845e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 494.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da834238a9c747cf9d86082fece88890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 388.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a1b453ad8445f0b9996ce0d75feaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/1333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 306.14\n",
      "Pretrained embeddings saved to: glove.vec\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(dataset, batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GloveModel(len(vocab), embedding_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        words, contexts, counts = [x.to(device) for x in batch]\n",
    "        # 提取batch内词、上下文的向量表示及偏置\n",
    "        word_embeds, word_biases = model.forward_w(words)\n",
    "        context_embeds, context_biases = model.forward_c(contexts)\n",
    "        # 回归目标值：必要时可以使用log(counts+1)进行平滑\n",
    "        log_counts = torch.log(counts)\n",
    "        # 样本权重\n",
    "        weight_factor = torch.clamp(torch.pow(counts / m_max, alpha), max=1.0) # clamp 将张量每个元素的范围限制到区间 [min,max]\n",
    "        optimizer.zero_grad()\n",
    "        # 计算batch内每个样本的L2损失\n",
    "        loss = (torch.sum(word_embeds * context_embeds, dim=1) + word_biases + context_biases - log_counts) ** 2\n",
    "        # 样本加权损失\n",
    "        wavg_loss = (weight_factor * loss).mean()\n",
    "        wavg_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += wavg_loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 合并词嵌入矩阵与上下文嵌入矩阵，作为最终的预训练词向量\n",
    "combined_embeds = model.w_embeddings.weight + model.c_embeddings.weight\n",
    "save_pretrained(vocab, combined_embeds.data, \"glove.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词义相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Query word: china\n",
      "cosine similarity=0.5030: recognized\n",
      "cosine similarity=0.4882: remodelling\n",
      "cosine similarity=0.4292: servotronics\n",
      "cosine similarity=0.4205: euromarket\n",
      "cosine similarity=0.4203: calmed\n",
      ">>> Query word: august\n",
      "cosine similarity=0.4885: trasaction\n",
      "cosine similarity=0.4709: withdrfawal\n",
      "cosine similarity=0.4534: fair\n",
      "cosine similarity=0.4355: 23rd\n",
      "cosine similarity=0.4251: maxtec\n",
      ">>> Query word: good\n",
      "cosine similarity=0.4810: corrientes\n",
      "cosine similarity=0.4663: ames\n",
      "cosine similarity=0.4612: peanut\n",
      "cosine similarity=0.4582: scenes\n",
      "cosine similarity=0.4505: pdvsa\n",
      ">>> Query word: paris\n",
      "cosine similarity=0.4307: au\n",
      "cosine similarity=0.4296: inn\n",
      "cosine similarity=0.4291: transcontinental\n",
      "cosine similarity=0.4253: kakuei\n",
      "cosine similarity=0.4238: dreyer\n"
     ]
    }
   ],
   "source": [
    "# 余弦相似度\n",
    "def knn(W, x, k):\n",
    "    # 计算查询向量x与矩阵W中每个行向量之间的余弦相似度\n",
    "    # 返回相似度最高的k个向量\n",
    "    similarities = torch.matmul(x, W.transpose(1, 0)) / (torch.norm(W, dim=1) * torch.norm(x) + 1e-9) # norm求范数，默认二范数\n",
    "    knn = similarities.topk(k=k)\n",
    "    return knn.values.tolist(), knn.indices.tolist()\n",
    "\n",
    "# 近义词检索\n",
    "def find_similar_words(embeds, vocab, query, k=5):\n",
    "    # 查询词也位于词向量空间中，它与自己的相似度最高(1.0)\n",
    "    # 取 k+1 个近邻\n",
    "    knn_values, knn_indices = knn(embeds, embeds[vocab[query]], k + 1)\n",
    "    knn_words = vocab.convert_ids_to_tokens(knn_indices)\n",
    "    print(f\">>> Query word: {query}\")\n",
    "    for i in range(k):\n",
    "        print(f\"cosine similarity={knn_values[i + 1]:.4f}: {knn_words[i + 1]}\")\n",
    "\n",
    "# 使用glove\n",
    "word_sim_queries = [\"china\", \"august\", \"good\", \"paris\"]\n",
    "vocab, embeds = load_pretrained(\"glove.vec\")\n",
    "for w in word_sim_queries:\n",
    "    find_similar_words(embeds, vocab, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类比性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Query: brother, sister, man\n",
      "['sister']\n",
      ">>> Query: paris, france, berlin\n",
      "['berlin']\n"
     ]
    }
   ],
   "source": [
    "def find_analogy(embeds, vocab, word_a, word_b, word_c):\n",
    "    vecs = embeds[vocab.convert_tokens_to_ids([word_a, word_b, word_c])]\n",
    "    x = vecs[2] + vecs[1] - vecs[0]\n",
    "    knn_values, knn_indices = knn(embeds, x, k=1)\n",
    "    analogies = vocab.convert_ids_to_tokens(knn_indices)\n",
    "    print(f\">>> Query: {word_a}, {word_b}, {word_c}\")\n",
    "    print(f\"{analogies}\")\n",
    "\n",
    "word_analogy_queries = [[\"brother\", \"sister\", \"man\"],\n",
    "                        [\"paris\", \"france\", \"berlin\"]]\n",
    "vocab, embeds = load_pretrained(\"glove.vec\")\n",
    "for w_a, w_b, w_c in word_analogy_queries:\n",
    "    find_analogy(embeds, vocab, w_a, w_b, w_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_polarity():\n",
    "    from nltk.corpus import sentence_polarity\n",
    "\n",
    "    vocab = Vocab.build(sentence_polarity.sents())\n",
    "\n",
    "    train_data = [(vocab.convert_tokens_to_ids(sentence), 0)\n",
    "                  for sentence in sentence_polarity.sents(categories='pos')[:4000]] \\\n",
    "        + [(vocab.convert_tokens_to_ids(sentence), 1)\n",
    "            for sentence in sentence_polarity.sents(categories='neg')[:4000]]\n",
    "\n",
    "    test_data = [(vocab.convert_tokens_to_ids(sentence), 0)\n",
    "                 for sentence in sentence_polarity.sents(categories='pos')[4000:]] \\\n",
    "        + [(vocab.convert_tokens_to_ids(sentence), 1)\n",
    "            for sentence in sentence_polarity.sents(categories='neg')[4000:]]\n",
    "\n",
    "    return train_data, test_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35890b56cc442f3bae2bc30b8879862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 166.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb4f2339e4e4e41a939cdca5edafa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 139.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8aba5f50bc4157a9c5f02ae488d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 103.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0fed29bd144eecb8ff0f6f9a97436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 72.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8871cfccc5804a7b9dbc989373b9b9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 46.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fed2dafa054f18ab4c1549fb47f806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 27.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6262b3863f743d0919a746d48e6d886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfde7a6b56384af1a89394eb3a1d0de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2189a6ca6ad1435fa53bb9bbb66ae698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c72fe9833549d6ad3ef64ac610b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae16d384b2a437cbe372d2d2b2c88a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/2662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "class BowDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "def collate_fn(examples):\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    offsets = [0] + [i.shape[0] for i in inputs]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    inputs = torch.cat(inputs)\n",
    "    return inputs, offsets, targets\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activate = F.relu\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, inputs, offsets):\n",
    "        embedding = self.embedding(inputs, offsets)\n",
    "        hidden = self.activate(self.linear1(embedding))\n",
    "        outputs = self.linear2(hidden)\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# tqdm是一个Python模块，能以进度条的方式显示迭代的进度\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_class = 2\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "\n",
    "# 加载数据\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(len(vocab), embedding_dim, hidden_dim, num_class)\n",
    "model.to(device) # 将模型加载到CPU或GPU设备\n",
    "\n",
    "#训练过程\n",
    "nll_loss = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 使用Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "        log_probs = model(inputs, offsets)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 测试过程\n",
    "acc = 0\n",
    "for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "    inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs, offsets)\n",
    "        acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "# 输出在测试集上的准确率\n",
    "print(f\"Acc: {acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab9424d55f0443cb8e8a7e3c9ca0c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 169.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c2bfb7558f460c870e4b854a88927c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 143.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450ae413e7814449ab5046245fbd31f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 111.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf50bed16b943f787a3fdae9470e43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 83.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4e4542745b4ce9be57b26dc4785e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 61.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c93eb7afb6c479d92f21fe763cf3c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 42.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b195618529140f1ade457af8029b2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 28.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0121089180402380ca353a510813a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 17.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec14ecf3faf4e82b4dc0b44ed9b192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204f841085a4e95b949b767dbc6387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e50762ed7a4a5aa6a611ba39a461d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/2662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "class BowDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "def collate_fn(examples):\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    offsets = [0] + [i.shape[0] for i in inputs]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    inputs = torch.cat(inputs)\n",
    "    return inputs, offsets, targets\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab, pt_vocab, pt_embeddings, hidden_dim, num_class):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        embedding_dim = pt_embeddings.shape[1] # 与预训练词向量维度保持一致\n",
    "        vocab_size = len(vocab)\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim) # 词向量层\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        for idx, token in enumerate(vocab.idx_to_token):\n",
    "            pt_idx = pt_vocab[token]\n",
    "            # 只初始化预训练词典中存在的词\n",
    "            # 对于未出现在预训练词典中的词，保持其随机初始化数量\n",
    "            if pt_idx != pt_vocab.unk:\n",
    "                self.embedding.weight[idx].data.copy_(pt_embeddings[pt_idx])\n",
    "\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activate = F.relu\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self, inputs, offsets):\n",
    "        embedding = self.embedding(inputs, offsets)\n",
    "        hidden = self.activate(self.linear1(embedding))\n",
    "        outputs = self.linear2(hidden)\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# tqdm是一个Python模块，能以进度条的方式显示迭代的进度\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_class = 2\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "\n",
    "# 加载数据\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pt_vocab, pt_embeddings = load_pretrained(\"glove.vec\")\n",
    "model = MLP(vocab, pt_vocab, pt_embeddings, hidden_dim, num_class)\n",
    "model.to(device) # 将模型加载到CPU或GPU设备\n",
    "\n",
    "#训练过程\n",
    "nll_loss = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 使用Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "        log_probs = model(inputs, offsets)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")\n",
    "\n",
    "# 测试过程\n",
    "acc = 0\n",
    "for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "    inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs, offsets)\n",
    "        acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "# 输出在测试集上的准确率\n",
    "print(f\"Acc: {acc / len(test_data_loader):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
