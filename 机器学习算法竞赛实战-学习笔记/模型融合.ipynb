{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型融合常常是竞赛取得胜利的关键！**   \n",
    "具有差异性的模型融合往往能给结果带来很大的提升。虽然并不是每次使用模型融合都能起到很大的作用，但是就平常的竞赛经验而言，尤其是在最终成绩相差不大的情况下，模型融合的方法往往会成为取胜的关键之一。   \n",
    "作者从**构建多样性**、**训练过程融合**和**训练结果融合**三部分介绍不同模型融合方法的应用场景。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 构建多样性  \n",
    "#### 1.1 特征多样性  \n",
    "构建多个有差异的特征集并分别建立模型，可使特征存在于不同的超空间，从而建立的多个模型有不同的泛化误差，最终模型融合时可以起到互补的效果。在竞赛中，队友之间的特征集往往是不一样的，在分数差异不大的情况下，直接进行模型融合基本会获得不错的收益。   \n",
    "另外，像随机森林的`max_features`，`XGBoost`中的`colsample_bytree`和`LightGBM`中的`feature_fraction`都是用来对训练集中的特征进行采样的，其实本质上就是构建特征的多样性。\n",
    "#### 1.2 样本多样性   \n",
    "样本多样性来自于不同的样本集，具体的做法是将数据集切分成多份，然后分别建立模型。我们知道很多树模型在训练时会进行采样，主要目的是防止过拟合，从而提升预测的准确性。   \n",
    "有时候将数据集切分为多份并不是随机进行的，而是根据具体的赛题数据进行切分，需要考虑如何切分可以构建最大限度的数据差异性，并用切分后的数据分别训练模型（2019年天池“全球城市计算AI挑战赛”）。   \n",
    "#### 1.3 模型多样性   \n",
    "不同模型对数据的表达能力是不同的，比如FM（Factorization Machine，因子分解机）能够学习到特征之间的交叉信息，并且记忆性较强；树模型可以很好的处理连续特征和离散特征（比如LightGBM和CatBoost），并且对异常值也具有很好的健壮性。把这两类在数据假设、表征能力方面有差异的模型融合起来肯定会达到一定的效果。\n",
    "   \n",
    "还有很多其他构建多样性的方法，比如训练目标多样性、参数多样性和损失函数选择的多样性等，都能产生非常好的效果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 训练过程融合    \n",
    "模型融合的方式有两种，第一种是**训练过程融合**，例如随机森林和`XGBoost`，这两种模型在训练种构造多个决策树进行融合，多个决策树可以看成是多个弱分类器，随机森林通过`Bagging`的方式进行融合，`XGboost`通过`Boosting`的方式进行融合。  \n",
    "#### 2.1 Bagging   \n",
    "`Bagging`从训练集中有放回的取出数据，这些数据构成样本集，这也保证了训练集的规模不变，然后用样本集训练弱分类器。重复上述过程多次，取平均值或者采用投票机制获取模型融合的最终结果。   \n",
    "`Bagging`通过减小误差之间的差来减少分类器的方差。换言之`Bagging`可以降低过拟合的风险。`Bagging`算法的效率来自于训练数据的不同，各模型之间存在很大的差异，并且在加权融合的过程中可使训练数据的错误相互抵消。   \n",
    "可以选择相同的分类器进行训练，也可以选择不同的分类器。   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
