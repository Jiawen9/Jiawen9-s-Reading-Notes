{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**决策树**（decision tree）基于特征对数据实例按照条件不断进行划分，最终达到分类或回归的目的。   \n",
    "本章作者主要介绍如何将决策树用于分类模型。   \n",
    "决策树模型预测的过程既可以看作一组if-then条件的集合，也可以视为定义在特征空间与类空间中的条件概率分布。   \n",
    "决策树模型的核心概念包括`特征选择方法`、`决策树构造过程`、`决策树剪枝`。常见的特征选择方法包括`信息增益`、`信息增益比`和`基尼指数（Gini index）`，对应的三种常见的决策树算法为`ID3`、`C4.5`和`CART`。   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从两种视角来理解决策树模型，第一种是将决策树看作一组if-then规则的集合，为决策树的根节点到叶子结点的每一条路径都构建一条规则，路径中的内部结点特征代表规则条件，而叶子结点表示这条规则的结论。一棵决策树所有的if-then规则都互斥且完备。if-then规则本质上是一组分类规则，决策树学习的目标就是基于数据归纳出这样一组规则。   \n",
    "第二种是从条件概率分布的角度来理解决策树。假设将特征空间划分为互不相交的区域，且每个区域定义的类的概率分布就构成了一个条件概率分布。决策树所表示的条件概率分布是由各个区域给定类的条件概率分布组成的。假设X为特征的随机变量，Y为类的随机变量，相应的条件概率分布表示为P(Y|X)，当叶子结点上的条件概率分布偏向某一类时，那么属于该类的概率就比较大。   \n",
    "我们的学习目标是找到一棵能够最大可能正确分类的决策树，但是为了保证泛化性，我们需要这棵决策树不能过于正确，在正则化参数的同时最小化经验误差。   \n",
    "决策树学习的目标就是最小化这个损失函数： \n",
    "$$ L_{a}\\left ( T \\right )  = \\sum_{t=1}^{\\left | T \\right | } N_{t}H_{t}\\left ( T \\right ) +\\alpha\\left | T \\right | $$\n",
    "H(t)是叶子结点上的经验熵（`empirical entropy`），a≥0为正则化参数，t为树T的叶子结点，每个叶子节点有Nt个样本。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 特征选择"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了构建一棵分类性能良好的决策树，我们需要从训练集中不断选取具有分类能力的特征。如果用一个特征对数据集进行分类的效果与随机选取的分类效果并无差异，我们可以认为该特征对数据集的分类能力是低下的；反之如果一个特征能够使得分类后的分支结点尽可能属于同一类别，即该结点有着较高的**纯度**（`purity`），那么该特征对数据集而言就具备较强的分类能力。   \n",
    "在决策树中，我们有三种方式来选取最优特征，包括`信息增益`、`信息增益比`和`基尼指数`。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 信息熵（`information entropy`）\n",
    "在信息论和概率统计中，熵是一种描述随机变量不确定性的度量方式，也可以用来描述样本集合的纯度，信息熵越低，样本不确定性越小，相应的纯度就越高。   \n",
    "假设当前样本数据集D中第k个类所占比例为Pk(k=1,2...Y)，那么该样本数据集的熵可定义为：\n",
    "$$ E\\left ( D \\right )=  -\\sum_{k=1}^{Y}p_{k}\\log_{}{p_{k}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402859586706309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 信息熵计算定义\n",
    "from math import log\n",
    "import pandas as pd\n",
    "\n",
    "# 信息熵计算函数\n",
    "def entropy(ele): # ele 包含类别取值的列表\n",
    "\n",
    "    probs = [ele.count(i) / len(ele) for i in set(ele)] # 计算列表中取值的概率分布\n",
    "    return -sum([prob * log(prob, 2) for prob in probs]) # 计算信息熵\n",
    "\n",
    "# 试运行\n",
    "df = pd.read_csv('./golf_data.csv')\n",
    "entropy(df['play'].to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 信息增益\n",
    "假设离散随机变量`(X，Y）`的联合概率分布为：   \n",
    "P（X=xi, Y=yi）= pij(i=1,2,...m,j=1,2,...n)   \n",
    "条件熵`E(Y|X)`表示在已知随机变量`X`的条件下`Y`的不确定度量，`E(Y|X)`可定义为在给定`X`的条件下`Y`的条件概率分布的熵对X的数学期望。条件熵可以表示为：\n",
    "$$ E\\left ( Y \\mid X \\right )=\\sum_{i=1}^{m}p_{i}E\\left ( Y\\mid X=x_{i}  \\right )  $$   \n",
    "在利用实际数据进行计算时，熵和条件熵中的概率计算都是基于极大似然估计得到，对应的熵和条件熵也叫做经验熵和经验条件熵。   \n",
    "**信息增益**（`information gain`） 定义为由于得到特征X的信息而使得类Y的信息不确定性减少的程度，即信息增益是一种描述目标类别确定性增加的量，特征的信息增益越大，目标类别的确定性越大。假设训练集D的经验熵为E(D)，给定特征A的条件下D的经验条件熵为E(D|A)，那么信息增益可定义为经验熵E(D)与经验条件熵E(D|A)之差：\n",
    "$$ g\\left ( D,A \\right ) =E\\left ( D \\right )-E\\left ( D\\mid A  \\right )  $$\n",
    "构建决策树算法时可以使用信息增益进行特征选择。给定训练集`D`和特征`A`，经验熵`E(D)`可以表示为对数据集`D`进行分类的不确定性，经验条件熵`E(D|A)`则表示在给定特征`A`之后对数据集`D`进行分类的不确定性，二者的差即为两个不确定性之间的差，也就是信息增益。具体到数据集`D`中，每个特征一般会有不同的信息增益，信息增益越大，代表对应的特征分类能力越强。`ID3`基于信息增益进行特征选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "def df_split(df, col): # 数据,特征\n",
    "    unique_col_val = df[col].unique() # 获取依据特征的不同取值\n",
    "    res_dict = {elem: pd.DataFrame for elem in unique_col_val}\n",
    "    for key in res_dict.keys():\n",
    "        res_dict[key] = df[:][df[col] == key]\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sunny':    humility outlook play  temp  windy\n",
       " 0      high   sunny   no   hot  False\n",
       " 1      high   sunny   no   hot   True\n",
       " 7      high   sunny   no  mild  False\n",
       " 8    normal   sunny  yes  cool  False\n",
       " 10   normal   sunny  yes  mild   True,\n",
       " 'overcast':    humility   outlook play  temp  windy\n",
       " 2      high  overcast  yes   hot  False\n",
       " 6    normal  overcast  yes  cool   True\n",
       " 11     high  overcast  yes  mild   True\n",
       " 12   normal  overcast  yes   hot  False,\n",
       " 'rainy':    humility outlook play  temp  windy\n",
       " 3      high   rainy  yes  mild  False\n",
       " 4    normal   rainy  yes  cool  False\n",
       " 5    normal   rainy   no  cool   True\n",
       " 9    normal   rainy  yes  mild  False\n",
       " 13     high   rainy   no  mild   True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = df_split(df, 'outlook')\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humility:0.15183550136234136\n",
      "outlook:0.2467498197744391\n",
      "windy:0.04812703040826927\n",
      "temp:0.029222565658954647\n"
     ]
    }
   ],
   "source": [
    "# 信息增益计算\n",
    "def info_gain(df, col):\n",
    "    res_dict = df_split(df, col)\n",
    "    entropy_D = entropy(df['play'].to_list()) # 计算数据集的经验熵\n",
    "    entropy_DA = 0 # 天气特征的经验条件熵\n",
    "    for key in res_dict.keys():\n",
    "        entropy_DA += len(res_dict[key]) / len(df) * entropy(res_dict[key]['play'].tolist()) # p * 经验条件熵\n",
    "    return entropy_D - entropy_DA # 天气特征的信息增熵\n",
    "\n",
    "# 增益越大，代表对应的特征分类能力越强\n",
    "print(f\"humility:{info_gain(df, 'humility')}\")\n",
    "print(f\"outlook:{info_gain(df, 'outlook')}\")\n",
    "print(f\"windy:{info_gain(df, 'windy')}\")\n",
    "print(f\"temp:{info_gain(df, 'temp')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 信息增益比\n",
    "信息增益是一种非常好的特征选择方法，但也存在一些问题：当某个特征分类取值较多时，该特征的信息增益计算结果就会较大，比如为数据集加一个“编号”特征，从第一条记录到最后一条记录，总共14个不同的取值，该特征会产生14个决策树分支，每个分支仅包含一个样本，每个结点的信息纯度都比较高，最后计算得到的信息增益也将远大于其他特征。但是，根据实际情况，我们知道“编号”这样的特征很难起到分类作用，这样构建出来的决策树是无效的，所以，基于信息增益选择特征时，会偏向取值较大的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humility:0.15183550136234136\n",
      "counter:0.9402859586706309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humility</th>\n",
       "      <th>outlook</th>\n",
       "      <th>play</th>\n",
       "      <th>temp</th>\n",
       "      <th>windy</th>\n",
       "      <th>counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>hot</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>no</td>\n",
       "      <td>cool</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>normal</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high</td>\n",
       "      <td>sunny</td>\n",
       "      <td>no</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal</td>\n",
       "      <td>sunny</td>\n",
       "      <td>yes</td>\n",
       "      <td>cool</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>normal</td>\n",
       "      <td>rainy</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>normal</td>\n",
       "      <td>sunny</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>high</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>normal</td>\n",
       "      <td>overcast</td>\n",
       "      <td>yes</td>\n",
       "      <td>hot</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high</td>\n",
       "      <td>rainy</td>\n",
       "      <td>no</td>\n",
       "      <td>mild</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   humility   outlook play  temp  windy  counter\n",
       "0      high     sunny   no   hot  False        0\n",
       "1      high     sunny   no   hot   True        1\n",
       "2      high  overcast  yes   hot  False        2\n",
       "3      high     rainy  yes  mild  False        3\n",
       "4    normal     rainy  yes  cool  False        4\n",
       "5    normal     rainy   no  cool   True        5\n",
       "6    normal  overcast  yes  cool   True        6\n",
       "7      high     sunny   no  mild  False        7\n",
       "8    normal     sunny  yes  cool  False        8\n",
       "9    normal     rainy  yes  mild  False        9\n",
       "10   normal     sunny  yes  mild   True       10\n",
       "11     high  overcast  yes  mild   True       11\n",
       "12   normal  overcast  yes   hot  False       12\n",
       "13     high     rainy   no  mild   True       13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"humility:{info_gain(df, 'humility')}\")\n",
    "# 为数据集加一个“编号”特征\n",
    "df['counter'] = range(len(df))\n",
    "print(f\"counter:{info_gain(df, 'counter')}\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用信息增益比对上述问题进行校正。特征`A`对数据集`D`的信息增益比可以定义为信息增益`g(D,A)`与数据集`D`关于特征`A`取值的熵 `EA(D)`的比值：n表示A的取值个数\n",
    "$$ g_{R}\\left ( D, A \\right )=\\frac{g\\left ( D, A \\right ) }{E_{A}\\left ( D \\right )} $$\n",
    "$$ E_{A} \\left ( D \\right )=-\\sum_{i=1}^{n}\\frac{\\left | D_{i}  \\right | }{D}\\log_{2}\\frac{\\left | D_{i}  \\right |}{D} $$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟信息增益一样，在基于信息增益比进行特征选择时，将信息增益比最大的特征作为决策树分裂结点。`C4.5`算法是基于信息增益比进行特征选择的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook:0.15642756242117517\n",
      "counter:0.2469656698468429\n"
     ]
    }
   ],
   "source": [
    "# 信息增益比计算\n",
    "def information_gain_ratio(df, col):\n",
    "    g = info_gain(df, col)\n",
    "    entropy_EAD = entropy(df[col].to_list())\n",
    "    return g / entropy_EAD\n",
    "\n",
    "# 试运行\n",
    "print(f\"outlook:{information_gain_ratio(df, 'outlook')}\")\n",
    "print(f\"counter:{information_gain_ratio(df, 'counter')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 基尼指数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了信息增益和信息增益比外，**基尼指数**(`Gini index`)也是一种较好的特征选择方法。基尼指数是针对概率分布而言的。假设样本有`K`个类，样本属于第`k`类的概率为pk，则该样本类别概率分布的基尼指数可定义为：\n",
    "$$ Gini\\left ( p \\right )=\\sum_{k=1}^{K}p_{k}\\left ( 1-p_{k}  \\right )=1-\\sum_{k=1}^{K}p_{k}^{2}   $$\n",
    "对于给定训练集`D`，`Ck`是属于第`k`类样本的集合，则该训练集的基尼指数可定义为：\n",
    "$$ Gini\\left ( D \\right )=1-\\sum_{k=1}^{K}\\left ( \\frac{\\left | C_{k}  \\right | }{\\left | D \\right | }  \\right ) ^{2}  $$\n",
    "如果训练集`D`根据特征`A`某一取值`a`划分为`D1`和`D2`两个部分，那么在特征`A`这个条件下，训练集`D`的基尼指数可定义为：\n",
    "$$ Gini\\left ( D,A \\right )=\\frac{D_{1}}{D}Gini\\left ( D_{1} \\right )+\\frac{D_{2}}{D}Gini\\left (D_{2} \\right) $$\n",
    "与信息熵定义类似，训练集`D`的基尼指数表示该集合的不确定性，`Gini(D,A)`表示训练集`D`经过`A=a`划分后的不确定性。对于分类任务而言，我们希望训练集的不确定性越小越好，对应的特征对训练样本的分类能力越强。`CART`算法是基于基尼指数进行特征选择的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunny:0.3936507936507937\n",
      "rainy:0.4571428571428572\n",
      "overcast:0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "# 计算基尼指数\n",
    "import numpy as np\n",
    "def calculate_gini(y): # y 包含类别取值的列表\n",
    "    # 将数组转化为列表\n",
    "    y = y.tolist()\n",
    "    probs = [y.count(i)/len(y) for i in np.unique(y)]\n",
    "    gini = sum([p*(1-p) for p in probs])\n",
    "    return gini\n",
    "\n",
    "# 划分数据集并计算基尼指数\n",
    "def gini_da(df, col, key): # g根据天气特征取值为晴与非晴划分为两个子集\n",
    "    col_val = [key, 'other']\n",
    "    new_dict = {elem: pd.DataFrame for elem in col_val} # 创建划分结果的数据框字典\n",
    "    new_dict[key] = df[:][df[col] == key]\n",
    "    new_dict['other'] = df[:][df[col] != key]\n",
    "    gini_DA = 0\n",
    "    for key in new_dict.keys():\n",
    "        gini_DA += len(new_dict[key]) / len(df) * calculate_gini(new_dict[key]['play'])\n",
    "    return gini_DA\n",
    "\n",
    "# 计算天气特征条件下数据集的基尼指数\n",
    "print(f\"sunny:{gini_da(df, 'outlook','sunny')}\")\n",
    "print(f\"rainy:{gini_da(df, 'outlook','rainy')}\")\n",
    "print(f\"overcast:{gini_da(df, 'outlook','overcast')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 决策树模型\n",
    "基于信息增益、信息增益比和基尼系数三种特征选择方法，分别有`ID3`、`C4.5`和`CART`三种经典的决策树算法。这三种算法在构造分类决策树时方法基本一致，都是通过特征选择方法递归地选择最优特征进行构造。其中`ID3`和`C4.5`算法只有决策树的生成，不包括决策树剪枝部分，所以这两种算法有时候容易过拟合。`CART`算法除用于分类外，还可以用于回归，并且该算法是包括决策树剪枝的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 ID3\n",
    "`ID3`算法的全称为`Iterative Dichotomiser 3`(3代迭代二叉树)，其核心是基于信息增益递归地选择最优特征构造决策树。   \n",
    "具体方法如下：首先预设一个决策树根结点，然后对所有特征计算信息增益，选择一个信息增益最大的特征作为最优特征，**根据该特征的不同取值建立子结点**，接着对每个子结点递归地调用上述方法，直到信息增益很小或没有特征可选时，即可构建最终的ID3决策树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2467498197744391,\n",
       " 'outlook',\n",
       " {'sunny':    humility outlook play  temp  windy\n",
       "  0      high   sunny   no   hot  False\n",
       "  1      high   sunny   no   hot   True\n",
       "  7      high   sunny   no  mild  False\n",
       "  8    normal   sunny  yes  cool  False\n",
       "  10   normal   sunny  yes  mild   True,\n",
       "  'overcast':    humility   outlook play  temp  windy\n",
       "  2      high  overcast  yes   hot  False\n",
       "  6    normal  overcast  yes  cool   True\n",
       "  11     high  overcast  yes  mild   True\n",
       "  12   normal  overcast  yes   hot  False,\n",
       "  'rainy':    humility outlook play  temp  windy\n",
       "  3      high   rainy  yes  mild  False\n",
       "  4    normal   rainy  yes  cool  False\n",
       "  5    normal   rainy   no  cool   True\n",
       "  9    normal   rainy  yes  mild  False\n",
       "  13     high   rainy   no  mild   True})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ID3算法的核心步骤-选择最优特征\n",
    "\n",
    "def choose_best_feature(df, label):\n",
    "    '''\n",
    "    思想：根据训练集和标签选择信息增益最大的特征作为最优特征\n",
    "    输入：\n",
    "    df:待划分的训练数据\n",
    "    label:训练标签\n",
    "    输出:\n",
    "    max_value:最大信息增益值\n",
    "    best_feature:最优特征\n",
    "    max_splited：根据最优特征划分后的数据字典\n",
    "    '''\n",
    "\n",
    "    entropy_D = entropy(df[label].tolist()) # 计算训练标签的信息熵\n",
    "    cols = [col for col in df.columns if col not in [label]] # 特征集\n",
    "    max_value, best_feature, max_splited = -999, None, None # 初始化最大信息增益、最优特征和划分后的数据集\n",
    "    for col in cols: # 遍历特征并根据特征取值进行划分\n",
    "        splited_set = df_split(df, col)\n",
    "        entropy_DA = 0 # 初始化经验条件熵\n",
    "        for subset_col, subset in splited_set.items():\n",
    "            entropy_DA += len(subset) / len(df) * entropy(subset[label].tolist()) # 计算当前特征的经验条件熵\n",
    "        info_gain = entropy_D - entropy_DA # 计算当前特征的特征增益\n",
    "        if info_gain > max_value: # 获取最大信息增熵，并保存对应的特征和划分结果 \n",
    "            max_value, best_feature = info_gain, col\n",
    "            max_splited = splited_set\n",
    "    return max_value, best_feature, max_splited\n",
    "\n",
    "# 试运行\n",
    "df = df.drop(labels='counter', axis=1)\n",
    "choose_best_feature(df, 'play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装构建ID3决策树的算法类\n",
    "\n",
    "class ID3Tree: # ID3算法类\n",
    "\n",
    "    class TreeNode: # 定义树结点\n",
    "        \n",
    "        def __init__(self, name): # 定义\n",
    "            self.name = name\n",
    "            self.connections = dict()\n",
    "            \n",
    "        def connect(self, label, node):\n",
    "            self.connections[label] = node\n",
    "\n",
    "    def __init__(self, df, label):\n",
    "        self.columns = df.columns\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "        self.root = self.TreeNode('Root')\n",
    "    \n",
    "    def construct_tree(self): # 构建树的调用\n",
    "        self.construct(self.root, '', self.df, self.columns)\n",
    "    \n",
    "    def construct(self, parent_node, parent_label, sub_df, columns): # 决策树构建方法\n",
    "        max_value, best_feature, max_splited = choose_best_feature(sub_df[columns], self.label) # 选择最优特征\n",
    "        if not best_feature: # 如果选不到最优特征，则构造单结点树\n",
    "            node = self.TreeNode(sub_df[self.label].iloc[0])\n",
    "            parent_node.connect(parent_label, node)\n",
    "            return\n",
    "        \n",
    "        # 根据最优特征以及子结点构建树\n",
    "        node = self.TreeNode(best_feature)\n",
    "        parent_node.connect(parent_label, node)\n",
    "\n",
    "        new_columns = [col for col in columns if col != best_feature] # 以A-Ag为新的特征集\n",
    "\n",
    "        # 递归的构造决策树\n",
    "        for splited_value, splited_data in max_splited.items():\n",
    "            self.construct(node, splited_value, splited_data, new_columns)\n",
    "    \n",
    "    # 打印树\n",
    "    def print_tree(self, node, tabs):\n",
    "        print(tabs + node.name)\n",
    "        for connection, child_node in node.connections.items():\n",
    "            print(tabs + \"\\t\" + \"(\" + str(connection) + \")\")\n",
    "            self.print_tree(child_node, tabs + \"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root\n",
      "\t()\n",
      "\t\toutlook\n",
      "\t\t\t(sunny)\n",
      "\t\t\t\thumility\n",
      "\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t(hot)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t\t\t(True)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t(cool)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(True)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t(overcast)\n",
      "\t\t\t\thumility\n",
      "\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t(hot)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(True)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t(cool)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(True)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(hot)\n",
      "\t\t\t\t\t\t\t\twindy\n",
      "\t\t\t\t\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t(rainy)\n",
      "\t\t\t\twindy\n",
      "\t\t\t\t\t(False)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t\t\t(cool)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\t\t\tyes\n",
      "\t\t\t\t\t(True)\n",
      "\t\t\t\t\t\thumility\n",
      "\t\t\t\t\t\t\t(normal)\n",
      "\t\t\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t\t\t(cool)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n",
      "\t\t\t\t\t\t\t(high)\n",
      "\t\t\t\t\t\t\t\ttemp\n",
      "\t\t\t\t\t\t\t\t\t(mild)\n",
      "\t\t\t\t\t\t\t\t\t\tno\n"
     ]
    }
   ],
   "source": [
    "# 构建id3决策树\n",
    "id3_tree = ID3Tree(df, 'play')\n",
    "id3_tree.construct_tree()\n",
    "id3_tree.print_tree(id3_tree.root, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 CART\n",
    "`CART`算法的全称为`classification and regression tree`(分类与回归树)，CART可以理解为在给定随机变量X的条件下输出随机变量Y的条件概率分布的学习算法。CART生成的决策树都是二叉决策树，内部节点取值为“是”和“否”，这种结点划分方法等价于递归地二分每个特征，将特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，即前述预测条件概率分布。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CART`算法也可以用来构建回归树。回归树对应特征空间的一个划分以及在该划分单元上的输出值。假设特征空间有`M`个划分单元`R1`，`R2`，...`RM`，且每个划分单元都有一个输出权重`cm`，那么回归树模型可以表示为：\n",
    "$$ f\\left ( x \\right )=\\sum_{m=1}^{M}c_{m}I\\left ( x\\in R_{m}  \\right ) $$\n",
    "和线性回归一样，回归树模型训练的目的同样是最小化均方损失，以期望求得最优输出权重`cm_hat`。具体而言，我们用平方误差最小方法求解每个单元上的最优权重，最优输出权重可以通过每个单元上所有输入实例对应的输出值的均值来确定：\n",
    "$$ \\hat{c_{m}}=average\\left ( y_{i} \\mid x_{i}\\in R_{m}\\right ) $$\n",
    "假设回归树选取第`j`个特征`x(j)`及其对应的某个取值`s`，将其作为划分特征和划分点，同时定义两个区域：\n",
    "$$ R_{1}\\left ( j,s \\right )=\\left \\{ x\\mid x^{(j)}\\le s  \\right \\};   R_{2}\\left ( j,s \\right )=\\left \\{ x\\mid x^{(j)}> s \\right \\} $$\n",
    "然后求解，可得到输入特征`j`和最优划分点`s`。\n",
    "$$ min_{js}\\left [ min_{c_{1}\\sum_{x_{i}\\in R_{1}(j,s)}^{}}(y_{i}-c_{1})^{2}+min_{c_{2}\\sum_{x_{i}\\in R_{2}(j,s)}^{}}(y_{i}-c_{2})^{2}\\right ]  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义树结点\n",
    "class TreeNode:\n",
    "    def __init__(self, feature_i=None, threshold=None, leaf_value=None, left_branch=None, right_branch=None):\n",
    "        \n",
    "        self.feature_i = feature_i # 特征索引\n",
    "        self.threshold = threshold # 特征划分阈值\n",
    "        self.leaf_value = leaf_value # 叶子节点取值\n",
    "        self.left_branch = left_branch # 左子树\n",
    "        self.right_branch = right_branch # 右子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义二叉特征分裂函数\n",
    "def feature_split(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_left = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_right = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_left, X_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义二叉决策树\n",
    "class BinaryDecisionTree:\n",
    "    def __init__(self, min_samples_split=3, min_gini_impurity=999, max_depth=float('inf'), loss=None): # 决策树初始参数\n",
    "        \n",
    "        self.root = None  # 根结点\n",
    "        self.min_samples_split = min_samples_split # 节点最小分裂样本数\n",
    "        self.mini_gini_impurity = min_gini_impurity # 节点初始化基尼不纯度\n",
    "        self.max_depth = max_depth # 树最大深度\n",
    "        self.impurity_calculation = None # 基尼不纯度计算函数\n",
    "        self._leaf_value_calculation = None # 叶子节点值预测函数\n",
    "        self.loss = loss # 损失函数\n",
    "    \n",
    "    def fit(self, X, y, loss=None): # 决策树拟合函数\n",
    "        self.root = self._build_tree(X, y) # 递归构建决策树\n",
    "        self.loss=None\n",
    "    \n",
    "    def _build_tree(self, X, y, current_depth=0): # 决策树构建函数\n",
    "        init_gini_impurity = 999 # 初始化最小基尼不纯度\n",
    "        best_criteria = None # 初始化最佳特征索引和阈值\n",
    "        best_sets = None # 初始化数据子集\n",
    "\n",
    "        Xy = np.concatenate((X, y), axis=1) # 合并输入和标签\n",
    "        n_samples, n_features = X.shape # 获取样本数和特征数\n",
    "        \n",
    "        # 设定决策树构建条件\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth: # 训练样本数量大于节点最小分裂样本数且当前树深度小于最大深度\n",
    "            \n",
    "            for feature_i in range(n_features):\n",
    "                unique_values = np.unique(X[:, feature_i]) # 获取第i个特征的唯一取值\n",
    "                \n",
    "                for threshold in unique_values: # 遍历取值并寻找最佳特征分裂阈值\n",
    "                    Xy1, Xy2 = feature_split(Xy, feature_i, threshold) # 特征节点二叉分裂\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0: # 如果分裂后的子集大小都不为0\n",
    "                        y1, y2 = Xy1[:, n_features:], Xy2[:, n_features:] # 获取两个子集的标签值\n",
    "                        impurity = self.impurity_calculation(y, y1, y2) # 计算基尼不纯度\n",
    "\n",
    "                        if impurity < init_gini_impurity:\n",
    "                            init_gini_impurity = impurity # 获取最小基尼不纯度\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold} # 最佳特征索引和分裂阈值\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   \n",
    "                                \"lefty\": Xy1[:, n_features:],   \n",
    "                                \"rightX\": Xy2[:, :n_features],  \n",
    "                                \"righty\": Xy2[:, n_features:]   \n",
    "                                }\n",
    "\n",
    "        if init_gini_impurity < self.mini_gini_impurity: # 如果计算的最小不纯度小于设定的最小不纯度\n",
    "            \n",
    "            # 分别构建左右子树\n",
    "            left_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            right_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return TreeNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\"threshold\"], left_branch=left_branch, right_branch=right_branch) \n",
    "\n",
    "        # 计算叶子计算取值\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "        return TreeNode(leaf_value=leaf_value)\n",
    "\n",
    "    def predict_value(self, x, tree=None): # 定义二叉树值预测函数\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.leaf_value is not None: # 如果叶子节点已有值，则直接返回已有值\n",
    "            return tree.leaf_value\n",
    "        \n",
    "        feature_value = x[tree.feature_i] # 选择特征并获取特征值\n",
    "\n",
    "        # 判断落入左子树还是右子树\n",
    "        branch = tree.right_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.left_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.left_branch\n",
    "        \n",
    "        return self.predict_value(x, branch) # 测试子集\n",
    "\n",
    "    def predict(self, X): # 数据集预测函数\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART分类树\n",
    "class ClassificationTree(BinaryDecisionTree): # 分类树\n",
    "    \n",
    "    def _calculate_gini_impurity(self, y, y1, y2): # 定义基尼不纯度计算过程\n",
    "        p = len(y1) / len(y)\n",
    "        gini_impurity = p * calculate_gini(y1) + (1-p) * calculate_gini(y2)\n",
    "        return gini_impurity\n",
    "    \n",
    "    def _majority_vote(self, y): # 多数投票\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # 统计多数\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "    \n",
    "    def fit(self, X, y): # 分类树拟合\n",
    "        self.impurity_calculation = self._calculate_gini_impurity\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\AppData\\Local\\Temp\\ipykernel_15796\\4187925257.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_left, X_right])\n"
     ]
    }
   ],
   "source": [
    "# 测试CART分类树\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = datasets.load_iris() # 鸢尾花数据集\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.3)\n",
    "clf = ClassificationTree()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART回归树\n",
    "class RegressionTree(BinaryDecisionTree): # 回归树\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = np.var(y, axis=0)\n",
    "        var_y1 = np.var(y1, axis=0)\n",
    "        var_y2 = np.var(y2, axis=0)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "        \n",
    "        variance_reduction = var_tot - (frac_1 * var_y1 + frac_2 * var_y2) # 计算方差减少量\n",
    "        return 1/sum(variance_reduction) # 方差减少量越大越好，所以取倒数\n",
    "\n",
    "    def _mean_of_y(self, y): # 节点值取平均\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\panda\\AppData\\Local\\Temp\\ipykernel_15796\\4187925257.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_left, X_right])\n",
      "C:\\Users\\panda\\AppData\\Local\\Temp\\ipykernel_15796\\1696106879.py:11: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return 1/sum(variance_reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.304654605263156\n"
     ]
    }
   ],
   "source": [
    "# 测试CART回归树\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "model = RegressionTree()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个完整的决策树算法，除决策树生成算法外，还包括决策树剪枝算法。决策树生成算法递归地产生决策树，生成的决策树大而全，但很容易过拟合。**决策树剪枝**(`pruning`)则是对已生成的决策树进行简化的过程，通过对已生成的决策树剪掉一些子树或者叶子结点，并将其根节点或父节点作为新的叶子结点，从而达到简化决策树的目的。   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树剪枝一般包括两种方法：**预剪枝**(`pre-pruning`)，**后剪枝**(`psot-pruning`)。所谓预剪枝就是在决策树生成过程中提前停止树的增长的一种剪枝算法。其主要思想是在决策树结点分裂之前，计算当前结点划分能否提升模型泛化能力，如果不能，则决策树在该结点停止生长。预剪枝提前停止树生长，一定程度上存在欠拟合的风险。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际应用中，我们还是以后剪枝方法为主。后剪枝主要是通过极小化决策树整体损失函数来实现。前面提到决策树最小化如下函数：\n",
    "$$ L_{a}\\left ( T \\right )  = \\sum_{t=1}^{\\left | T \\right | } N_{t}H_{t}\\left ( T \\right ) +\\alpha\\left | T \\right | $$\n",
    "其中第一项的经验熵可以表示为：\n",
    "$$ H_{t}(T)=-\\sum_{k}^{}\\frac{N_{tk}}{N_{t}}log\\frac{N_{tk}}{N_{t}} $$\n",
    "`L(T)`的第一项可以表示为：\n",
    "$$ L_{a}\\left ( T \\right )  = \\sum_{t=1}^{\\left | T \\right | } N_{t}H_{t}\\left ( T \\right )=-\\sum_{t=1}^{\\left | T \\right |}\\sum_{k=1}^{K} N_{tk}log\\frac{N_{tk}}{N_{t}}  $$\n",
    "改写决策树优化函数为\n",
    "$$ L_{a} (T)=L(T)+\\alpha\\left | T \\right |  $$\n",
    "`L(T)`是模型的经验误差项，`|T|`是决策树复杂度，`α`是正则化参数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树后剪枝就是在复杂度`α`确定的情况下，选择损失函数`Lα(T)`最小的决策树模型。给定生成算法得到的决策树`T`和正则化参数`α`，决策树后剪枝算法描述如下：   \n",
    "(1).计算每个树结点的经验熵`Ht(T)`。   \n",
    "(2).递归地自底向上回缩，假设一组叶子结点回缩到父结点前后的树分别为`Tbefore`和`Tafter`，其对应的损失函数分别为`Lα(Tbefore)`和`Lα(Tafter)`，如果`Lα(Tafter)`≤`Lα(Tbefore)`，则进行剪枝，将父结点变为新的叶子结点。   \n",
    "(3).重复(2)，直到得到损失函数最小的子树`Tα`。   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CART后剪枝`通过计算子树的损失函数来实现剪枝并得到一个子树序列，然后通过`交叉验证`的方法从子树序列中选取最优子树。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 基于sklearn实现分类树和回归树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 基于sklearn实现分类树\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "data = datasets.load_iris() # 鸢尾花数据集\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.3)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 14.449605263157896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 基于sklearn实现回归树\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X, y = load_boston(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
