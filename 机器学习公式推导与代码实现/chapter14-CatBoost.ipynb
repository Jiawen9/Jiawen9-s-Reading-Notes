{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林\n",
    "`Bagging`是区别于`Boosting`的一种集成学习框架，通过对数据集自身采样来获取不同子集，并且对每个子集训练基分类器来进行模型集成。   \n",
    "\n",
    "`Bagging`是一种并行化的集成学习方法。`随机森林`就是`Bagging`学习框架的一个代表，通过`样本`和`特征`两个随机性来构造基分类器，由多棵决策树进而形成随机森林。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Bagging\n",
    "前面几章提到的集成学习模型都是Boosting框架，通过不断地迭代和残差拟合的方式来构造集成的树模型。Bagging作为并行式集成学习方法最典型的框架，其核心概念在于`自助采样`(`bootstrap sampling`)。给定包含m个样本的数据大小的采样集，**有放回**的随机抽取一个样本放入样本集中，经过m次采样，可得到一个与原始数据集一样大小的采样集。最终可以采样得到T个包含m个样本的采样集，然后基于每个采样集训练出一个基分类器，最后将这些基分类器进行组合。这就是Bagging的主要思想。  \n",
    " \n",
    "Bagging最大的特征就是可以并行实现，Boosting则是一种序列迭代的实现方式。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 随机森林的基本原理\n",
    "`随机森林`(`random forest, RF`)以决策树为分类器进行集成，进一步在决策树训练过程中引入了随机选择数据集特征的方法，故称为随机森林。   \n",
    "简单来说随机森林的算法过程就是`两个随机性`:   \n",
    "- 假设有M个样本，有放回地随机选择M个样本。   \n",
    "- 假设有N个特征，在决策时每个结点要进行分裂时，随机从这N个特征中选取n个特征(n<<N)，从这n个特征中选择特征进行节点分裂。\n",
    "   \n",
    "最后构建大量决策树组成随机森林，然后将每棵树地结果进行综合（分类可使用投票法，回归可以使用均值法）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 随机森林地算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree.cart import ClassificationTree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
