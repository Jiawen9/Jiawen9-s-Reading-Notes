{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林\n",
    "`Bagging`是区别于`Boosting`的一种集成学习框架，通过对数据集自身采样来获取不同子集，并且对每个子集训练基分类器来进行模型集成。   \n",
    "\n",
    "`Bagging`是一种并行化的集成学习方法。`随机森林`就是`Bagging`学习框架的一个代表，通过`样本`和`特征`两个随机性来构造基分类器，由多棵决策树进而形成随机森林。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Bagging\n",
    "前面几章提到的集成学习模型都是Boosting框架，通过不断地迭代和残差拟合的方式来构造集成的树模型。Bagging作为并行式集成学习方法最典型的框架，其核心概念在于`自助采样`(`bootstrap sampling`)。给定包含m个样本的数据大小的采样集，**有放回**的随机抽取一个样本放入样本集中，经过m次采样，可得到一个与原始数据集一样大小的采样集。最终可以采样得到T个包含m个样本的采样集，然后基于每个采样集训练出一个基分类器，最后将这些基分类器进行组合。这就是Bagging的主要思想。  \n",
    " \n",
    "Bagging最大的特征就是可以并行实现，Boosting则是一种序列迭代的实现方式。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 随机森林的基本原理\n",
    "`随机森林`(`random forest, RF`)以决策树为分类器进行集成，进一步在决策树训练过程中引入了随机选择数据集特征的方法，故称为随机森林。   \n",
    "简单来说随机森林的算法过程就是`两个随机性`:   \n",
    "- 假设有M个样本，有放回地随机选择M个样本。   \n",
    "- 假设有N个特征，在决策时每个结点要进行分裂时，随机从这N个特征中选取n个特征(n<<N)，从这n个特征中选择特征进行节点分裂。\n",
    "   \n",
    "最后构建大量决策树组成随机森林，然后将每棵树地结果进行综合（分类可使用投票法，回归可以使用均值法）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 随机森林地算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cart import ClassificationTree\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, min_samples_split=2, min_gain=999, max_depth=float(\"inf\"), max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_gini_impurity = min_gain\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "        # 基于决策树构造森林\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = ClassificationTree(\n",
    "                min_samples_split=self.min_samples_split, \n",
    "                min_gini_impurity=self.min_gini_impurity, \n",
    "                max_depth=self.max_depth\n",
    "            )\n",
    "            self.trees.append(tree)\n",
    "        \n",
    "    def bootstrap_sampling(self, X, y):\n",
    "\n",
    "        X_y = np.concatenate([X, y.reshape(-1, 1)], axis=1) # 合并数据输入和标签\n",
    "        np.random.shuffle(X_y) # 打乱数据\n",
    "        n_samples = X_y.shape[0] # 样本量\n",
    "        sampling_subsets = [] # 初始化抽样子集列表\n",
    "        # 遍历产生多个抽样子集\n",
    "        for _ in range(self.n_estimators):\n",
    "            # 第一个随机性，行抽样\n",
    "            idx1 = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            bootstrap_Xy = X_y[idx1, :]\n",
    "            bootstrap_X = bootstrap_Xy[:, :-1]\n",
    "            bootstrap_y = bootstrap_Xy[:, -1]\n",
    "            sampling_subsets.append([bootstrap_X, bootstrap_y])\n",
    "        return sampling_subsets\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 对森林中每棵树训练一个双随机抽样子集\n",
    "        sub_sets = self.bootstrap_sampling(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        # 设置max_feature\n",
    "        if self.max_features == None:\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        \n",
    "        # 遍历拟合每棵树\n",
    "        for i in range(self.n_estimators):\n",
    "            sub_X, sub_y = sub_sets[i]\n",
    "            # 第二个随机性，列抽样\n",
    "            idx2 = np.random.choice(n_features, self.max_features, replace=True)\n",
    "            sub_X = sub_X[:, idx2]\n",
    "            self.trees[i].fit(sub_X, sub_y)\n",
    "            self.trees[i].feature_indices = idx2\n",
    "            print(f'the {i}th tree is trained done')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = []\n",
    "        for i in range(self.n_estimators):\n",
    "            idx = self.trees[i].feature_indices\n",
    "            sub_X = X[:, idx]\n",
    "            y_pred = self.trees[i].predict(sub_X)\n",
    "            y_preds.append(y_pred)\n",
    "\n",
    "        y_preds = np.array(y_preds).T\n",
    "        res = []\n",
    "        for j in y_preds:\n",
    "            res.append(np.bincount(j.astype('int')).argmax())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Jiawen9-s-Reading-Notes\\机器学习公式推导与代码实现\\chapter15-随机森林\\utils.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([X_left, X_right])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0th tree is trained done\n",
      "the 1th tree is trained done\n",
      "the 2th tree is trained done\n",
      "the 3th tree is trained done\n",
      "the 4th tree is trained done\n",
      "the 5th tree is trained done\n",
      "the 6th tree is trained done\n",
      "the 7th tree is trained done\n",
      "the 8th tree is trained done\n",
      "the 9th tree is trained done\n",
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape) # 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "rf = RandomForest(n_estimators=10, max_features=15)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.基于sklearn地随机森林算法实现\n",
    "基于随机森林的分类和回归调用方式分别为`ensemble.RandomForestClassifier`和`ensemble.RandomForestRegressor`。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
